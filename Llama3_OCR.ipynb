{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soberbichler/Notebooks4Historical_Newspapers/blob/main/Llama3_OCR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DLFqCYQVFcC"
      },
      "source": [
        "# Researching German Historical Newspapers with Llama AI Model\n",
        "## Example: OCR Post-Correction\n",
        "\n",
        "Created by Sarah Oberbichler [![ORCID](https://info.orcid.org/wp-content/uploads/2019/11/orcid_16x16.png)](https://orcid.org/0000-0002-1031-2759)\n",
        "\n",
        "This notebook shows how LLMs can be used to support research with historical newspapers. In this example, the Llama 3 model is used to to correct OCR of previously OCR'd historical newspapers pages.\n",
        "\n",
        "OCR quality has been a long-standing issue in digitization efforts. Historical newspapers are particularly affected due their complexity, historical fonts, or degradation. Additionally, OCR technology faced limitations when dealing with historical scripts.\n",
        "\n",
        "\n",
        "### 1.   Query the German Historical Newspaper Portal\n",
        "\n",
        "German historical newspapers from the German Digital Library can be accessed via the DDB-API. This API is open access and allows to query the Historical Newspapers available in the German Newspaper Portal ([Deutsches Zeitungsportal](https://https://www.deutsche-digitale-bibliothek.de/newspaper)). An instruction, provided by the German Newspaper Portal (from Karl Krägerlin), can be found [here](https://https://deepnote.com/app/karl-kragelin-b83c/Zeitungsportal-API-d9224dda-8e26-4b35-a6d7-40e9507b1151)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9ysGwj_KLa-"
      },
      "outputs": [],
      "source": [
        "# @markdown #####  Launch this cell and get access to the API of the Newspaper Portal from the German Digital Library\n",
        "!pip install ddbapi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzW_y_D7ffhD"
      },
      "outputs": [],
      "source": [
        "# @markdown ####  Import the necessary packages\n",
        "import pandas as pd\n",
        "from ddbapi import zp_issues, zp_pages, list_column, filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fe5WdrIsfkoR"
      },
      "outputs": [],
      "source": [
        "# @markdown ### Possible kwargs for the functions are:\n",
        "# @markdown - language: Use ISO Codes, currently ger, eng, fre, spa\n",
        "# @markdown - place_of_distribution: Search inside \"Verbreitungsort\"\n",
        "# @markdown - use a list for multiple search-words\n",
        "# @markdown - publication_date: Get newspapers by publication date.\n",
        "# @markdown - zdb_id: Search by ZDB-ID\n",
        "# @markdown - provider: Search by Data Provider\n",
        "# @markdown - paper_title: Search inside the title of the Newspaper\n",
        "# @markdown - plainpagefulltex: search inside the OCR\n",
        "# Get the data\n",
        "# Get the data\n",
        "df = zp_pages(\n",
        "    publication_date='[1906-01-01T12:00:00Z TO 1906-12-31T12:00:00Z]',\n",
        "    plainpagefulltext=[\"Rückwanderer*\"],\n",
        "    #paper_title='Deutsche allgemeine Zeitung'\n",
        "    )\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "it6-W30BYP6G"
      },
      "outputs": [],
      "source": [
        "# @markdown #### We can narrow down the text surrounding the keyword in order to reduce the input tokens for the model.\n",
        "def extract_context(keywords, text, tokens_before, tokens_after):\n",
        "    # Finde alle Positionen der Keywords im Text\n",
        "    keyword_positions = []\n",
        "\n",
        "    for keyword in keywords:\n",
        "        keyword_start = text.find(keyword)\n",
        "        while keyword_start != -1:\n",
        "            keyword_positions.append(keyword_start)\n",
        "            keyword_start = text.find(keyword, keyword_start + len(keyword))\n",
        "\n",
        "    if not keyword_positions:\n",
        "        return \"Keywords not found in text.\"\n",
        "\n",
        "    # Bestimme den Start- und Endpunkt des Kontextfensters\n",
        "    first_occurrence = min(keyword_positions)  # Erstes Vorkommen eines Keywords\n",
        "    last_occurrence = max(keyword_positions)  # Letztes Vorkommen eines Keywords\n",
        "\n",
        "    # Berechne den Start des Kontextfensters\n",
        "    start_index = max(0, first_occurrence - tokens_before)\n",
        "\n",
        "    # Berechne das Ende des Kontextfensters\n",
        "    end_index = min(len(text), last_occurrence + tokens_after)\n",
        "\n",
        "    # Extrahiere das Kontextfenster\n",
        "    context = text[start_index:end_index]\n",
        "\n",
        "    return context\n",
        "\n",
        "# Liste von Keywords\n",
        "keywords = [\"ückwander\"]\n",
        "\n",
        "df['context'] = [extract_context(keywords, row['plainpagefulltext'], 2000, 3000) for _, row in df.iterrows()]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHyn5ihQfx8r"
      },
      "outputs": [],
      "source": [
        "# @markdown #### Save the results as Excel file\n",
        "df.to_excel('newspaper_rückkehrer.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KLdlJqpYxrd"
      },
      "source": [
        "## Setting up the requirements for the Llama model\n",
        "\n",
        "Llama 3 is a family of models developed by Meta. Llama 3 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cFFKynv2-y4"
      },
      "outputs": [],
      "source": [
        "pip install replicate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNyDsn1d3FxF"
      },
      "outputs": [],
      "source": [
        "# @markdown ##### Get an API key at https://replicate.com/, activate the billing, save your key as .env file. To do so, take following steps:\n",
        "# @markdown - Open a Notepad and write REPLICATE_API_TOKEN = \"your key\"\n",
        "# @markdown - Click on Save option and change the file type to 'All files'\n",
        "# @markdown - Keep the file name as .env.\n",
        "# @markdown - Hit Save Now the file is an .env file.\n",
        "\n",
        "\n",
        "!pip install python-dotenv\n",
        "\n",
        "import os\n",
        "import dotenv\n",
        "\n",
        "#Set the REPLICATE_API_TOKEN environment variable\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHvstS-_eX48"
      },
      "outputs": [],
      "source": [
        "# @markdown Load the .env file into the drive/MyDrive\n",
        "dotenv.load_dotenv('/content/drive/MyDrive/.env')\n",
        "\n",
        "os.getenv('REPLICATE_KEY_TOKEN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeZ7GcEkQ3n8"
      },
      "source": [
        "# Run model for OCR-post correction\n",
        "\n",
        "To run OCR-post correction, it is essential to formulate a precise prompt. For example, it needs to be specified that the whole text should be corrected, while summarizations and any other addition need to be avoided. A guide on how to write effective prompts can be found also [here](https://https://support.google.com/a/users/answer/14200040?hl=en).\n",
        "\n",
        "Depending on the size of the dataframe, it can take a while to load."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuIdx6Unkhwc"
      },
      "outputs": [],
      "source": [
        "df\n",
        "df = df[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WZFw4ST0DDM"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import replicate\n",
        "\n",
        "def OCR_correction(newspaper_page):\n",
        "    # Define the prompt for separating articles\n",
        "\n",
        "    input = {\n",
        "    \"prompt\": f\"Korrogiere OCR Fehler des gesamten deutschen Textes und drucke den gesamten korrigierten Text \\n\\n{newspaper_page}\\n\\n---\\n\\ .\",\n",
        "    \"prompt_template\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are an OCR correction expert. Please don't ask for feedback or questions <|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
        "    \"max_new_tokens\": 8000,\n",
        "    }\n",
        "\n",
        "    # Initialize an empty string to collect the response\n",
        "    text = \"\"\n",
        "\n",
        "    # Generate the response using the LLaMA model\n",
        "    for event in replicate.stream(\n",
        "        \"meta/meta-llama-3-70b-instruct\",\n",
        "        input=input\n",
        "    ):\n",
        "        if event:\n",
        "            text += str(event)\n",
        "        else:\n",
        "            print(\"Received empty event data\")\n",
        "\n",
        "    # Return the separated articles\n",
        "    return text\n",
        "\n",
        "# Create an empty list to store the separated articles\n",
        "post_OCR = []\n",
        "\n",
        "# Loop through each row in the dataframe\n",
        "for index, row in df.iterrows():\n",
        "    # Extract the text of the newspaper page from the current row\n",
        "    newspaper_page = row['context']\n",
        "\n",
        "    # Separate articles for the current newspaper page only if newspaper_page is not empty\n",
        "    if newspaper_page.strip():\n",
        "        text = OCR_correction(newspaper_page)\n",
        "\n",
        "        # Append the separated articles to the list, even if it’s empty\n",
        "        post_OCR.append(text)\n",
        "    else:\n",
        "        print(\"Skipping empty newspaper page\")\n",
        "\n",
        "# Add the list of separated articles as a new column 'article' in the dataframe\n",
        "df['article_corrected'] = post_OCR\n",
        "\n",
        "# Print the modified dataframe\n",
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evI1JCMWHPqv"
      },
      "outputs": [],
      "source": [
        "df['article_corrected'] = df['article_corrected'].apply(lambda x: x.split('\\n\\n', 1)[1].lstrip() if isinstance(x, str) and '\\n\\n' in x else x)\n",
        "df['article_corrected'] = df['article_corrected'].apply(lambda x: x.split('\\n\\n', 1)[1].lstrip() if isinstance(x, str) and '\\n\\n' in x else x)\n",
        "\n",
        "df.to_excel('article_corrected.xlsx', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkhoJBe74V0labOq7ROHhY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
